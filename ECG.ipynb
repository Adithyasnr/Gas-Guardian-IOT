{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adithyasnr/Gas-Guardian-IOT/blob/main/ECG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3WnRCOyb2w1",
        "outputId": "b9e45376-1b47-4591-9801-3c20c59cfa83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Found 91 files in '/content/drive/My Drive/ECG ID'\n",
            "Person_85\n",
            "Person_84\n",
            "Person_88\n",
            "Person_81\n",
            "Person_83\n",
            "Person_82\n",
            "Person_86\n",
            "Person_90\n",
            "Person_87\n",
            "Person_89\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the folder inside Google Drive\n",
        "folder_path = \"/content/drive/My Drive/ECG ID\"\n",
        "\n",
        "# Check if the folder exists\n",
        "if os.path.exists(folder_path):\n",
        "    files = os.listdir(folder_path)  # List all files\n",
        "    print(f\"✅ Found {len(files)} files in '{folder_path}'\")\n",
        "\n",
        "    for file in files[:10]:  # Print only first 10 files as a preview\n",
        "        print(file)\n",
        "else:\n",
        "    print(\"❌ Folder not found! Check the path.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wfdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUns5MSrfdMs",
        "outputId": "a2243a82-2a15-4c9e-ef72-ebd14e2845e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.26.4)\n",
            "Collecting pandas>=2.2.3 (from wfdb)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.14.1)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2025.1.31)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Downloading wfdb-4.2.0-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, wfdb\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3 wfdb-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wfdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, filtfilt\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, BatchNormalization, Dropout"
      ],
      "metadata": {
        "id": "jzWyYdcfJW7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyWavelets\n",
        "import pywt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBP7T0AlKCgl",
        "outputId": "a5659302-e854-4de0-d1d8-a7e7c2063af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (1.26.4)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ecg_data(record_path):\n",
        "    record = wfdb.rdrecord(record_path)\n",
        "    annotation = wfdb.rdann(record_path, 'atr')\n",
        "    ecg_signal = record.p_signal[:, 0]\n",
        "    r_peaks = annotation.sample\n",
        "\n",
        "    # Apply baseline drift removal and bandpass filter\n",
        "    ecg_signal = remove_baseline_drift(ecg_signal)\n",
        "    ecg_signal = bandpass_filter(ecg_signal, low_freq=0.5, high_freq=40, fs=500)\n",
        "\n",
        "    return ecg_signal, r_peaks"
      ],
      "metadata": {
        "id": "zcM4hKgGJaLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_baseline_drift(signal, wavelet=\"db6\", level=9):\n",
        "    coeff = pywt.wavedec(signal, wavelet, level=level)\n",
        "    coeff[0] = np.zeros_like(coeff[0])  # Remove baseline drift\n",
        "    reconstructed_signal = pywt.waverec(coeff, wavelet)\n",
        "    return reconstructed_signal"
      ],
      "metadata": {
        "id": "eqhnS2msJkOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bandpass_filter(signal, low_freq, high_freq, fs, order=5):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = low_freq / nyquist\n",
        "    high = high_freq / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    filtered_signal = filtfilt(b, a, signal)\n",
        "    return filtered_signal"
      ],
      "metadata": {
        "id": "mxZE_-waJmK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_segment(signal, r_peaks, window_size=180):\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    signal_normalized = scaler.fit_transform(signal.reshape(-1, 1)).flatten()\n",
        "\n",
        "    segments = []\n",
        "    half_window = window_size // 2\n",
        "    for r_peak in r_peaks:\n",
        "        start = max(r_peak - half_window, 0)\n",
        "        end = min(r_peak + half_window, len(signal_normalized))\n",
        "        if end - start == window_size:\n",
        "            segments.append(signal_normalized[start:end])\n",
        "    return np.array(segments)"
      ],
      "metadata": {
        "id": "_hjTVa9NJn3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_all_records(base_directory, visualize=False):\n",
        "    all_segments = []\n",
        "    all_labels = []\n",
        "    person_dirs = [d for d in os.listdir(base_directory) if os.path.isdir(os.path.join(base_directory, d))]\n",
        "    lis =  ['Person_01', 'Person_02','Person_09', 'Person_10' ]\n",
        "    print(person_dirs)\n",
        "    for person_id in lis:\n",
        "        person_path = os.path.join(base_directory, person_id)\n",
        "        for record_file in os.listdir(person_path):\n",
        "            if record_file.endswith('.dat'):\n",
        "                record_base = record_file[:-4]\n",
        "                record_path = os.path.join(person_path, record_base)\n",
        "                ecg_signal, r_peaks = load_ecg_data(record_path)\n",
        "                segments = normalize_and_segment(ecg_signal, r_peaks)\n",
        "                all_segments.extend(segments)\n",
        "                all_labels.extend([person_id] * len(segments))\n",
        "                if visualize and person_id == 'Person_01':  # Example visualization for one person\n",
        "                    visualize_ecg_signals(ecg_signal, segments, r_peaks, record_base)\n",
        "\n",
        "    return np.array(all_segments), np.array(all_labels)"
      ],
      "metadata": {
        "id": "PxQhYH1WJpsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop"
      ],
      "metadata": {
        "id": "tt1keSA9Jsmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_segments, all_labels = load_and_process_all_records(folder_path)\n",
        "\n",
        "# Encoding labels\n",
        "label_encoder = LabelEncoder()\n",
        "all_labels_encoded = label_encoder.fit_transform(all_labels)\n",
        "\n",
        "# Prepare data for the LSTM model\n",
        "X = np.array(all_segments).reshape((len(all_segments), -1, 1))\n",
        "y = np.array(all_labels_encoded)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the BiLSTM model\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dropout(0.2),  # Adding dropout for regularization\n",
        "    BatchNormalization(),  # Normalize the activations from the LSTM layer\n",
        "\n",
        "    Bidirectional(LSTM(512, return_sequences=True)),\n",
        "    # Additional LSTM layer with sequence return\n",
        "    Dropout(0.2),  # More dropout for regularization\n",
        "    BatchNormalization(),  # More normalization\n",
        "     Bidirectional(LSTM(256, return_sequences=True)),\n",
        "    # Additional LSTM layer with sequence return\n",
        "    Dropout(0.2),  # More dropout for regularization\n",
        "    BatchNormalization(),\n",
        "     Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    # Additional LSTM layer with sequence return\n",
        "    Dropout(0.2),  # More dropout for regularization\n",
        "    BatchNormalization(),\n",
        "    Bidirectional(LSTM(64)),  # Final LSTM layer without sequence return\n",
        "#     Dropout(0.2),  # Dropout after final LSTM layer\n",
        "    Dense(128, activation='relu'),  # An additional Dense layer for more complex transformations\n",
        "#     Dropout(0.2),  # Dropout after Dense layer\n",
        "    Dense(len(np.unique(y_train)), activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "optimizer = RMSprop(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, validation_split=0.2, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "xqqDXwt3JxBE",
        "outputId": "35a3ce7e-f751-44c6-8d05-258b4065fdaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Person_85', 'Person_84', 'Person_88', 'Person_81', 'Person_83', 'Person_82', 'Person_86', 'Person_90', 'Person_87', 'Person_89', 'Person_75', 'Person_79', 'Person_71', 'Person_72', 'Person_73', 'Person_77', 'Person_78', 'Person_76', 'Person_74', 'Person_80', 'Person_63', 'Person_66', 'Person_69', 'Person_70', 'Person_64', 'Person_61', 'Person_62', 'Person_67', 'Person_68', 'Person_65', 'Person_51', 'Person_54', 'Person_53', 'Person_56', 'Person_52', 'Person_58', 'Person_60', 'Person_57', 'Person_55', 'Person_59', 'Person_43', 'Person_49', 'Person_50', 'Person_47', 'Person_42', 'Person_41', 'Person_48', 'Person_45', 'Person_46', 'Person_44', 'Person_37', 'Person_38', 'Person_34', 'Person_36', 'Person_33', 'Person_40', 'Person_31', 'Person_32', 'Person_39', 'Person_35', 'Person_27', 'Person_21', 'Person_30', 'Person_25', 'Person_24', 'Person_28', 'Person_26', 'Person_29', 'Person_23', 'Person_22', 'Person_13', 'Person_20', 'Person_15', 'Person_14', 'Person_12', 'Person_11', 'Person_17', 'Person_16', 'Person_19', 'Person_18', 'Person_10', 'Person_06', 'Person_05', 'Person_09', 'Person_08', 'Person_01', 'Person_02', 'Person_07', 'Person_04', 'Person_03']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-ffd4985498ba>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_segments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_process_all_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Encoding labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_labels_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d9df4193290c>\u001b[0m in \u001b[0;36mload_and_process_all_records\u001b[0;34m(base_directory, visualize)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mrecord_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mrecord_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mecg_signal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_peaks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ecg_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_and_segment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mecg_signal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_peaks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mall_segments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-13bcc36d8993>\u001b[0m in \u001b[0;36mload_ecg_data\u001b[0;34m(record_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_ecg_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwfdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwfdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdann\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'atr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mecg_signal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_signal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mr_peaks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wfdb/io/annotation.py\u001b[0m in \u001b[0;36mrdann\u001b[0;34m(record_name, extension, sampfrom, sampto, shift_samps, pn_dir, return_label_elements, summarize_labels)\u001b[0m\n\u001b[1;32m   1948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;31m# Read the file in byte pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1950\u001b[0;31m     \u001b[0mfilebytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_byte_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpn_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m     \u001b[0;31m# Get WFDB annotation fields from the file bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wfdb/io/annotation.py\u001b[0m in \u001b[0;36mload_byte_pairs\u001b[0;34m(record_name, extension, pn_dir)\u001b[0m\n\u001b[1;32m   2090\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpn_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2091\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2092\u001b[0;31m             \u001b[0mfilebytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"<u1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2093\u001b[0m     \u001b[0;31m# PhysioNet file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/My Drive/ECG ID/ecg_identification_model_bilstm.h5')\n",
        "model.save('/content/ecg_identification_model_bilstm.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxrAaNN4LrrU",
        "outputId": "bf436886-b794-4b5c-e44c-17f0cc5e941d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#  `y_test` are the true labels and `y_pred` are the predicted labels from model\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7JFdHQVNIrY",
        "outputId": "75becf2d-5fbc-4860-b9b2-cd6c824bc5d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 70ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Person_01       0.92      1.00      0.96        79\n",
            "   Person_02       0.87      0.89      0.88        84\n",
            "   Person_09       0.92      0.85      0.88        26\n",
            "   Person_10       0.90      0.67      0.77        27\n",
            "\n",
            "    accuracy                           0.90       216\n",
            "   macro avg       0.90      0.85      0.87       216\n",
            "weighted avg       0.90      0.90      0.90       216\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9CXzkAqIye-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "random_index = random.randint(0, len(X_test) - 1)\n",
        "random_sample = X_test[random_index]\n",
        "true_class = y_test[random_index]\n",
        "\n",
        "predicted_probs = model.predict(np.expand_dims(random_sample, axis=0))\n",
        "predicted_class = np.argmax(predicted_probs)\n",
        "\n",
        "predicted_person = label_encoder.inverse_transform([predicted_class])[0]\n",
        "true_person = label_encoder.inverse_transform([true_class])[0]\n",
        "\n",
        "print(f\"Random Sample Index: {random_index}\")\n",
        "if predicted_person in [\"Person_01\", \"Person_02\", \"Person_09\", \"Person_10\"]:\n",
        "    print(\"Authentication Success\")\n",
        "else:\n",
        "    print(\"Authentication failed\")\n",
        "print(\"WELCOME \",predicted_person)\n",
        "print(\"\\n\")\n",
        "print(f\"Predicted Class: {predicted_class} ({predicted_person})\")\n",
        "print(f\"True Class: {true_class} ({true_person})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4VnqsGRP7mL",
        "outputId": "cf1f61a6-efc1-48d6-cc3e-ebdafacda336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "Random Sample Index: 39\n",
            "Authentication Success\n",
            "WELCOME  Person_10\n",
            "\n",
            "\n",
            "Predicted Class: 3 (Person_10)\n",
            "True Class: 3 (Person_10)\n"
          ]
        }
      ]
    }
  ]
}